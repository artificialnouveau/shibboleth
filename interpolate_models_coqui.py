# -*- coding: utf-8 -*-
"""Interpolate_Models_Coqui.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12NO-FUMVvla1MKouLOcVWNDwJgfjZ_E5
"""

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/Projects/R2201ArtificialVoice/checkpoints/

!pip install TTS
!pip install pydub

import torch
import os
import io
from copy import deepcopy



def interpolate_models(model1_path, model2_path, output_path, alpha):
    # Load model state dictionaries
    model1_state_dict = torch.load(model1_path, map_location=torch.device('cuda'))['model']
    model2_state_dict = torch.load(model2_path, map_location=torch.device('cuda'))['model']

    # Make sure the model architectures are the same
    assert model1_state_dict.keys() == model2_state_dict.keys()

    # Create a new state dict in which each weight is an interpolation of the weights of the two models
    new_state_dict = deepcopy(model1_state_dict)
    for key in model1_state_dict.keys():
        if key in model2_state_dict:
            # Only interpolate weights; ignore things like running mean/var in batch norm
            if isinstance(model1_state_dict[key], torch.nn.Parameter) or isinstance(model1_state_dict[key], torch.Tensor):
                new_state_dict[key] = alpha * model1_state_dict[key] + (1 - alpha) * model2_state_dict[key]

    # Save the new model state_dict
    torch.save({'model': new_state_dict}, output_path)




# Usage
for i in range(11):
  alpha = str(i/10)
  print(alpha)
  text = f'"This is a test sentence with an alpha of {alpha}"'  # note the double quotes around the actual text
  out_path = f"/content/drive/MyDrive/Projects/R2201ArtificialVoice/generated_audio/output_{i}.wav"
  model_path = f"/content/drive/MyDrive/Projects/R2201ArtificialVoice/checkpoints/interpolated_model_alpha_{i}.pth"
  config_path = "/content/drive/MyDrive/Projects/R2201ArtificialVoice/checkpoints/config.json"  # replace this with the actual path to your configuration file
  !tts --text {text} --model_path {model_path} --config_path {config_path} --out_path {out_path}

import os
from pydub import AudioSegment
import random
import numpy as np
import subprocess

model_paths = ['/content/drive/MyDrive/Projects/R2201ArtificialVoice/checkpoints/effi_checkpoint_160000.pth',
               '/content/drive/MyDrive/Projects/R2201ArtificialVoice/checkpoints/amir_checkpoint_300000.pth']
config_path = "/content/drive/MyDrive/Projects/R2201ArtificialVoice/checkpoints/config.json"
output_path = "/content/drive/MyDrive/Projects/R2201ArtificialVoice/generated_audio/"

def merge_audio_files(audio_files, output_file):
    combined = AudioSegment.empty()
    for file in audio_files:
        combined += AudioSegment.from_wav(file)
    print(f'Saving file to {output_file}')
    combined.export(output_file, format='wav')

def generate_speech(text, model_path, config_path, output_path):
    cmd = f"tts --text '{text}' --model_path {model_path} --config_path {config_path} --out_path {output_path}"
    subprocess.call(cmd, shell=True)

def generate_sentence(sentence, model_paths, config_path):
    words = sentence.split()
    audio_files = []
    overlap_flag = False
    overlap_audio = None

    for i in range(0, len(words), np.random.randint(2,5)):
        phrase = ' '.join(words[i:i+np.random.randint(2,5)])
        output_path = f"output_{phrase.replace(' ','')}.wav"
        model_path = random.choice(model_paths)
        generate_speech(phrase, model_path, config_path, output_path)
        audio = AudioSegment.from_wav(output_path)

        if overlap_flag:
            audio = audio.overlay(overlap_audio)
            overlap_flag = False

        # Randomly select if the next phrase will be spoken by both models
        if random.choice([True, False]):
            overlap_flag = True
            model_path = model_paths[0] if model_path == model_paths[1] else model_paths[1]  # Select the other model
            output_path_overlap = f"output_{phrase.replace(' ','')}_overlap.wav"
            generate_speech(phrase, model_path, config_path, output_path_overlap)
            overlap_audio = AudioSegment.from_wav(output_path_overlap)

        audio.export(output_path, format='wav')
        audio_files.append(output_path)

    # Combine all audios
    merge_audio_files(audio_files, "/content/drive/MyDrive/Projects/R2201ArtificialVoice/generated_audio/efam_random_remix5.wav")

# Assuming model_paths and config_path are defined
sentence = "This is a random remix test between effis and amirs voices Do you believe in life after love"
generate_sentence(sentence, model_paths, config_path)

